Explain the need of Flume.
Ans: Apache Flume is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of
streaming data into the Hadoop Distributed File System (HDFS). It has a simple and flexible architecture based on streaming data flows; 
and is robust and fault tolerant with tunable reliability mechanisms for failover and recovery.
YARN coordinates data ingest from Apache Flume and other services that deliver raw data into an Enterprise Hadoop cluster.
It is a service for streaming logs into Hadoop

Event: A byte payload with optional string headers that represent the unit of data that Flume can transport from it’s point of origination
to it’s final destination.
Flow: Movement of events from the point of origin to their final destination is considered a data flow, or simply flow. This is not a 
rigorous definition and is used only at a high level for description purposes.
Client: An interface implementation that operates at the point of origin of events and delivers them to a Flume agent. Clients typically
operate in the process space of the application they are consuming data from. For example, Flume Log4j Appender is a client.
Agent: An independent process that hosts flume components such as sources, channels and sinks, and thus has the ability to receive, store
and forward events to their next-hop destination.
Source: An interface implementation that can consume events delivered to it via a specific mechanism. For example, an Avro source is a
source implementation that can be used to receive Avro events from clients or other agents in the flow. When a source receives an event, 
it hands it over to one or more channels.
Channel: A transient store for events, where events are delivered to the channel via sources operating within the agent. An event put in 
a channel stays in that channel until a sink removes it for further transport. An example of channel is the JDBC channel that uses a 
file-system backed embedded database to persist the events until they are removed by a sink. Channels play an important role in ensuring
durability of the flows.
Sink: An interface implementation that can remove events from a channel and transmit them to the next agent in the flow, or to the event’s 
final destination. Sinks that transmit the event to it’s final destination are also known as terminal sinks. The Flume HDFS sink is an
example of a terminal sink. Whereas the Flume Avro sink is an example of a regular sink that can transmit messages to other agents that 
are running an Avro source.

Explain the working of Flume and its components in brief.
Ans:Flume basically consists of three main components, Source, Channel and Sink. A flume agent is basically a JVM process which consists
of these three components through which data flow occurs. Given below is representation of a simple Flume agent listening to a webserver 
and writing the data to HDFS.

The source component receives the data from an external data source or a flume sink. There are different formats in which the data can be
transferred. Therefore the source has to be configured based on the format of the input data. The source can be configured to listen on 
various sources. This helps in aggregating data from various sources and store them in a single location. The channel is a temporary 
storage for the events. It is similar to a queue before being passed to the sink. The storage can be of two types, memory storage or 
disk storage. By using memory based storage high throughput can be achieved but in case of a failure all the data is lost. A disk based
storage offers less throughput and more reliability.

The sink retrieves the events from the channel and then either writes it into a file system or passes it to the next agent. The source
and sink work asynchronously which leads to the necessity of channel. The architecture of the Flume agent is very flexible. Source
component can be designed to listen to various sources. Same goes with the channel and sink. Various sinks can be configured to send 
data to various destinations. We can develop a multi-hop architecture for the data transfer which is a combination of various flume 
agents linked either to aggregate data from various sources or distribute data to various destinations.

The flume system follows a transaction approach for the data transfer. An event is requested from the channel by the sink. An event
from the channel is sent to the sink. The event is removed from the channel after it receives a confirmation from the sink. It helps
in reliable transfer and prevents data loss. The same is followed in between sink-source in case of multi-hop system. Flume supports 
various types of data formats/mechanisms which can be passed through its agents. It supports Avro, Thrift, Syslog and Netcat. These 
are various formats used in data transfer.
